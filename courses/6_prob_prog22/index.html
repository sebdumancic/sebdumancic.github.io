<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Probabilistic Programming Seminar 2022-2023 | Sebastijan Dumancic</title> <meta name="author" content="Sebastijan Dumancic"> <meta name="description" content="Research seminar on Probabilistic programming in the TU Delft's CS Master programme"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://sebdumancic.github.io/courses/6_prob_prog22/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Sebastijan </span>Dumancic</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">team</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Probabilistic Programming Seminar 2022-2023</h1> <p class="post-description">Research seminar on Probabilistic programming in the TU Delft's CS Master programme</p> </header> <article> <h2 id="description">Description</h2> <p>Probabilistic programming languages (PPLs) use the syntax and semantics of programming languages to define probabilistic models. Using computer programs as a representation of probabilistic models yields two benefits. First, any computable process can be modelled as a probabilistic program. Second, PPLs enable a diverse audience – data scientists, systems designers, medical doctors, etc. – to design and reason about probabilistic systems. PPLs are becoming one of the central topics in probabilistic reasoning and programming languages, with increasing interest from industry and academia.</p> <p>This course will:</p> <ol> <li>introduce the core ideas of probabilistic programming – probabilistic inference, language design, and applications – as well as state-of-the-art ideas in the field that make PPLs more reliable, usable, and faster.</li> <li>teach you how to critically analyse state-of-the-art ideas, their strengths and weaknesses, and propose improvements.</li> </ol> <p>At the end of the course, you will be able to:</p> <ul> <li>read and write probabilistic programs</li> <li>understand how probabilistic programming languages are implemented and how they can be extended</li> <li>understand the basic and state-of-the-art probabilistic inference procedures (which are applicable beyond probabilistic programs)</li> </ul> <h2 id="prerequisites">Prerequisites</h2> <p>The target audience for this course are Master’s and PhD students. The are no formal prerequisites, but the students are expected to be comfortable with programming and mathematical notation. Basic familiarity with probability theory, artificial intelligence and machine learning is expected (equivalent to a Bachelor’s level course). We will not revise these topics during the lectures, but refresher materials are listed below.</p> <h2 id="course-format">Course Format</h2> <p>The course will consist of lectures by the instructors, paper reviews, student presentations on research papers and a project. The points are distributed as follows:</p> <ul> <li>60%: Presentations and reviews. The points include: <ul> <li>one paper presentation (40% of the category),</li> <li>paper reviews and class participation (60%)</li> </ul> </li> <li>40%: Project.</li> </ul> <p><strong>Paper reviews.</strong> The course will take the format of a research seminar. This means that there will be no textbook and blackboard lectures; instead, we will be reading and discussing state of the art papers in the field. You are expected to read and review every paper before the class. The paper reviews will not be extensive: they consist of a few questions to answer regarding the main idea of the paper and are there to help you study.</p> <p><strong>Paper presentation.</strong> You are expected to present one of the assigned papers. You will understand the paper in details, including searching for additional literature that helps you to understand the paper and why it works, and present it to your colleagues.</p> <p><strong>Class participation.</strong> As this is a seminar course, you are expected to <em>actively participate</em> in class. You cannot pass the course by only submitting the reviews and the project without saying anything during lectures. See <a href="#how-to-do-well-in-a-seminar-course">how to do well in a seminar course.</a></p> <p><strong>Project.</strong> The project will take the form of a competition. You compete in groups of 2 or 3 with the goal to write the fastest inference procedure for the given problems.</p> <p>For more details about the grading scheme, see <a href="#grading-scheme">here</a>.</p> <h2 id="schedule">Schedule</h2> <p>The materials are split in two categories:</p> <ul> <li> <strong>Required literature</strong>: these are the paper we cover in class. They form the mandatory literature.</li> <li> <strong>If you want to know more</strong>: these papers are not mandatory reading, but rather interesting references you can use to further study the topic.</li> </ul> <p>You do not have to read every paper from the required literature. You will be divided in groups (group split is on Brightspace) and each group reads only one paper for the class. You will learn about the other paper from the presentation of your colleagues.</p> <p>Each lecture comes with a reading guide. This guide explains what you should get from the papers (and what is perhaps not relevant!). Use it to focus your efforts on important stuff. Of course, you are not discouraged to go into depths of every paper.</p> <style scoped="">table{font-size:13px}</style> <table> <thead> <tr> <th>Lecture <br> and presenters</th> <th style="text-align: left">Topic and readings</th> </tr> </thead> <tbody> <tr> <td>W1 L1 <br> September 5 <br><br> Sebastijan</td> <td style="text-align: left"> <strong>Topic</strong> <br> Introduction to probabilistic programming. Thinking generatively. What kind of problems can be solved with probabilistic programming? The anatomy of a probabilistic program. <br><a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/Lecture1.pdf">[Slides]</a> <br><br> <strong>Required readings</strong> (not before class) <br> Chapters 1-3 <a href="https://probmods.org/index.html" rel="external nofollow noopener" target="_blank">Probabilistic Models of Cognition</a> <br> Chapter 1 from <a href="https://arxiv.org/pdf/1809.10756.pdf" rel="external nofollow noopener" target="_blank">An introduction to probabilistic programming</a> <br><br> <strong>Reading guide</strong> <br> Focus on model-based reasoning and how it contrasts to machine learning. Understand what ‘thinking generatively’ means and work out some examples. Understand how probabilistic programming differs from standard programming, how its key elements – sample and observe – influence its execution, and how to interpret the program as a density.</td> </tr> <tr> <td>W1 L2 <br> September 8 <br><br> Sebastijan</td> <td style="text-align: left"> <strong>Topic</strong> <br> Basic inference procedures: Enumeration, Rejection sampling, Importance sampling, Metropolis-Hastings MCMC, Particle filtering. Why do they work? <br><a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/Lecture2.pdf">[Slides]</a> <br><br> <strong>Required readings</strong> <br> <a href="https://probmods.org/chapters/inference-algorithms.html" rel="external nofollow noopener" target="_blank">Chapter 8</a> from <a href="https://probmods.org" rel="external nofollow noopener" target="_blank">Probabilistic models of cognition</a> <br> Sections 4.1-4.3 from <a href="https://arxiv.org/pdf/1809.10756.pdf" rel="external nofollow noopener" target="_blank">An introduction to probabilistic programming</a> <br><br> <strong>Reading guide</strong> <br> ProbMods chapter will give you nice intuition of the inference algorithms. Focus on understanding the role of sample and observe statements in each inference algorithm. Introduction to ProbProg chapter will formally describe all the algorithms. Directly connect the algorithms to how we interpret programs as densities, and why do they work. While reading this chapter, ignore the parts about inference on graphs, Aside notes, Addressing transformation, and section 4.3.5.</td> </tr> <tr> <td>W2 L3 <br> September 12 <br><br> </td> <td style="text-align: left">No lecture. Read papers for the next one.</td> </tr> <tr> <td>W2 L4 <br> September 15 <br><br> Group discussion</td> <td style="text-align: left"> <strong>Topic</strong><br> How do we implement probabilistic programs? <br><br> <strong>Required literature</strong><br> <a href="https://arxiv.org/pdf/1509.02151.pdf" rel="external nofollow noopener" target="_blank">C3: Lightweight Incrementalized MCMC for Probabilistic Programs using Continuations and Callsite Caching</a> <br> <a href="http://proceedings.mlr.press/v15/wingate11a/wingate11a.pdf" rel="external nofollow noopener" target="_blank">Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation</a> <br> Sections 6.1, 6.4-6.7 from <a href="https://arxiv.org/pdf/1809.10756.pdf" rel="external nofollow noopener" target="_blank">An introduction to probabilistic programming</a> <br> <br> <strong>Reading guide</strong><br> Distill the main idea and don’t get stuck on implementation details if the paper discusses it (e.g., data structures used, etc.). What is the main computational (program execution/transformation) principle? Why is it useful? How does it interact with the sample and observe statements? Build up one of the inference algorithms in all 3 paradigms. <br><br><strong>If you want to know more</strong><br><a href="https://dl.acm.org/doi/10.1145/3064899.3064910" rel="external nofollow noopener" target="_blank">Design and Implementation of Probabilistic Programming Language Anglican</a><br><a href="https://dl.acm.org/doi/pdf/10.1145/3314221.3314642" rel="external nofollow noopener" target="_blank">Gen: a general-purpose probabilistic programming system with programmable inference</a> <br> <a href="https://arxiv.org/pdf/2002.02702.pdf" rel="external nofollow noopener" target="_blank">DynamicPPL: Stan-like Speed for Dynamic Probabilistic Models</a> <br> <a href="https://arxiv.org/pdf/1507.00996.pdf" rel="external nofollow noopener" target="_blank">A New Approach to Probabilistic Programming Inference</a> <br> <a href="https://www.denotational.co.uk/publications/scibior-kammar-ghahramani-funcitonal-programming-for-modular-bayesian-inference.pdf" rel="external nofollow noopener" target="_blank">Functional Programming for Modular Bayesian Inference</a> <br><a href="https://pyro.ai/examples/minipyro.html" rel="external nofollow noopener" target="_blank">MiniPyro</a> </td> </tr> <tr> <td>W3 L5 <br> September 19 <br><br> Gautham <br> Jaap</td> <td style="text-align: left"> <strong>Topic</strong><br> How do we automatically improve the performance of PPs through rewrites? <br><br> <strong>Required literature</strong><br><a href="https://sf.snu.ac.kr/publications/r2.pdf" rel="external nofollow noopener" target="_blank">R2: An Efficient MCMC Sampler for Probabilistic Programs</a> <a href="#w3-l5-p1">[Review form]</a> <a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/2022-2023/ProbProg-R2.pdf">[Slides]</a> <br> <a href="https://proceedings.mlr.press/v119/gorinova20a/gorinova20a.pdf" rel="external nofollow noopener" target="_blank">Automatic Reparameterisation of Probabilistic Programs</a> <a href="#w3-l5-p2">[Review form]</a> <a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/2022-2023/W3L5.pdf">[Slides]</a> <br><br> <strong>Reading guide</strong><br> Focus on understanding the transformations. Ignore the references to specific inference algorithms, unless the algorithm is one mentioned in W1L2. You can skip the proofs, and glance over things like ELBO (which we cover later). Do the transformations affect sample or observe statements? In which way? Give an example how the transformation improves inference. <br><br> <strong>If you want to know more</strong><br> <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/slicing.pdf" rel="external nofollow noopener" target="_blank">Slicing Probabilistic Programs</a> <br> <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/final.pdf" rel="external nofollow noopener" target="_blank">A Provably Correct Sampler for Probabilistic Programs</a> <br> <a href="https://arxiv.org/pdf/1509.02962.pdf" rel="external nofollow noopener" target="_blank">Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs</a> <br> <a href="https://dl.acm.org/doi/pdf/10.1145/3296979.3192399" rel="external nofollow noopener" target="_blank">Incremental Inference for Probabilistic Programs</a> <br><a href="https://arxiv.org/pdf/2010.11887.pdf" rel="external nofollow noopener" target="_blank">Conditional independence by typing</a> <br> <a href="http://proceedings.mlr.press/v33/yang14d.pdf" rel="external nofollow noopener" target="_blank">Generating Efficient MCMC Kernels from Probabilistic Programs</a> <br><br> <strong>Additional resources that can help you</strong> <br> <a href="https://www.cs.williams.edu/~freund/cs326/ReasoningPart1.html#summary-so-far-rules-for-finding-the-weakest-precondition" rel="external nofollow noopener" target="_blank">Weakest precondition analysis</a> </td> </tr> <tr> <td>W3 L6 <br> September 22 <br> <br>Jord<br> Wei</td> <td style="text-align: left"> <strong>Topic</strong><br> Hamiltonian Monte Carlo – better inference through gradient estimation <br><br> <strong>Required literature</strong><br> <a href="https://www.jmlr.org/papers/volume18/17-468/17-468.pdf" rel="external nofollow noopener" target="_blank">Automatic differentiation and Machine Learning: A Survey</a>. <a href="#w3-l6-p1">[Review form]</a> <a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/2022-2023/automatic_differentiation_jord%20.pdf">[Slides]</a> <br> <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="external nofollow noopener" target="_blank">Pattern Recognition and Machine Learning, Chapter 11.5</a> <a href="#w3-l6-p2">[Review form]</a> <br><br> <strong>Reading guide</strong><br> When reading about HMC, focus on understanding the intuition of the approach and then go to the math. Make sure you understand why this is a good idea. Make sure you understand what each of the two steps brings to the table. For automated differentiation, understand the properties of each of the modes and pay attention how to implement them. <br><br> <strong>If you want to know more</strong> <br><a href="http://www.stat.columbia.edu/~gelman/research/published/nuts.pdf" rel="external nofollow noopener" target="_blank">The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo</a> <br> <a href="https://arxiv.org/abs/1810.04449" rel="external nofollow noopener" target="_blank">Faster Hamiltonian Monte Carlo by Learning Leapfrog Scale</a> <br> <a href="https://dritchie.github.io/pdf/hmc.pdf" rel="external nofollow noopener" target="_blank">Generating Design Suggestions under Tight Constraints with Gradient-based Probabilistic Programming</a> <br> <a href="http://proceedings.mlr.press/v139/mak21a/mak21a.pdf" rel="external nofollow noopener" target="_blank">Nonparametric Hamiltonian Monte Carlo</a> <br> <a href="https://arxiv.org/pdf/1206.1901.pdf" rel="external nofollow noopener" target="_blank">MCMC using Hamiltonian dynamics</a> <br><br> <strong>Other materials that could help you better understand the topic</strong><br> <a href="https://arxiv.org/pdf/1701.02434.pdf" rel="external nofollow noopener" target="_blank">A conceptual introduction to Hamiltonian Monte Carlo</a> <br> <a href="https://github.com/ColCarroll/minimc" rel="external nofollow noopener" target="_blank">Simple HMC implementation</a> <br> <a href="https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/" rel="external nofollow noopener" target="_blank">Building a better Markov Chain</a> <br> <a href="https://willcrichton.net/notes/probabilistic-programming-under-the-hood/" rel="external nofollow noopener" target="_blank">Probabilistic programming under the hood</a> <br> <a href="https://github.com/MikeInnes/diff-zoo" rel="external nofollow noopener" target="_blank">Differentiation for Hackers</a> <br> <a href="https://towardsdatascience.com/build-your-own-automatic-differentiation-program-6ecd585eec2a" rel="external nofollow noopener" target="_blank">Build your own automated differentiation program</a> <br> <a href="https://www.youtube.com/watch?v=ZGtezhDaSpM" rel="external nofollow noopener" target="_blank">Alan Maloney’s HMC for Dummies</a> </td> </tr> <tr> <td>W4 L7 <br> September 26 <br><br> Calin <br> Mark</td> <td style="text-align: left"> <strong>Topic</strong><br> Scaling up inference by surrogate distributions <br><br> <strong>Required literature</strong><br><a href="https://arxiv.org/pdf/1301.1299.pdf" rel="external nofollow noopener" target="_blank">Automated Variational Inference in Probabilistic Programming</a> <a href="#w4-l7-p1">[Review form]</a> <a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/2022-2023/W4L7p1.pdf">[Slides]</a> <br><a href="https://papers.nips.cc/paper/2015/file/352fe25daf686bdb4edca223c921acea-Paper.pdf" rel="external nofollow noopener" target="_blank">Automatic Variational Inference in Stan</a> <a href="#w4-l7-p2">[Review form]</a> <a href="https://sebdumancic.github.io/assets/pdf/teaching/probprog/2022-2023/W4L7p2.pdf">[Slides]</a> <br><br> <strong>Reading guide</strong><br> Variational inference is an important topics when we think about scaling probabilistic inference to large real world problems. Make sure to understand what does variational inference exactly do; if stuck, check Bishop’s textbook. For the papers you are reading, focus on understanding what are they trying to achieve, how do they manage to automatically derive variational distributions from a program. Why do they do it in this way? <br><br><strong>If you want to know more</strong><br> <a href="https://arxiv.org/pdf/1601.00670.pdf" rel="external nofollow noopener" target="_blank">Variational Inference: A Review for Statisticians</a> <br> <a href="https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf" rel="external nofollow noopener" target="_blank">Stochastic Variational Inference</a> <br><a href="https://arxiv.org/pdf/1603.00788.pdf" rel="external nofollow noopener" target="_blank">Automatic Differentiation Variational Inference</a> <br> <a href="https://arxiv.org/pdf/1907.08827.pdf" rel="external nofollow noopener" target="_blank">Towards Verified Stochastic Variational Inference for Probabilistic Programs</a>. <br><br> <strong>Additional materials that could help you better understand the topic</strong><br> <a href="https://www.youtube.com/watch?v=ogdv_6dbvVQ" rel="external nofollow noopener" target="_blank">David Blei’s lecture on the foundations of variational inference</a> <br> <a href="https://www.ritchievink.com/blog/2019/09/16/variational-inference-from-scratch/" rel="external nofollow noopener" target="_blank">Variational inference from scratch</a> </td> </tr> <tr> <td>W4 L8 <br> September 29 <br><br> Siert <br>Konstantinos</td> <td style="text-align: left"> <strong>Topic</strong><br> Reusing previous inferences to speed up future ones <br><br> <strong>Required literature</strong><br><a href="https://arxiv.org/pdf/1610.05735.pdf" rel="external nofollow noopener" target="_blank">Deep Amortized Inference for Probabilistic Programs</a> [<a href="#w4-l7-p1">Review form</a>] <br> <a href="https://arxiv.org/pdf/1610.09900.pdf" rel="external nofollow noopener" target="_blank">Inference Compilation and Universal Probabilistic Programming</a> [<a href="#w4-l7-p2">Review form</a>]<br><br> <strong>Reading guide</strong><br> <br><br> <strong>If you want to know more</strong><br> <a href="https://proceedings.mlr.press/v151/naderiparizi22a/naderiparizi22a.pdf" rel="external nofollow noopener" target="_blank">Amortized Rejection Sampling in Universal Probabilistic Programming</a> <br> <a href="http://proceedings.mlr.press/v130/liang21a/liang21a.pdf" rel="external nofollow noopener" target="_blank">Accelerating Metropolis-Hastings with Lightweight Inference Compilation</a> </td> </tr> <tr> <td>W5 L9 <br> October 3 <br><br> Lucas <br>Matei</td> <td style="text-align: left"> <strong>Topic</strong><br> Programmable inference <br><br> <strong>Required literature</strong><br> <a href="https://www.cantab.net/users/yutian.chen/Publications/MansinghkaEtAl_pldi18.pdf" rel="external nofollow noopener" target="_blank">Probabilistic programming with programmable inference</a> [<a href="#w5-l9-p1">Review form</a>] <br> <a href="https://arxiv.org/pdf/1801.03612.pdf" rel="external nofollow noopener" target="_blank">Using probabilistic programs as proposals</a> [<a href="#w5-l9-p2">Review form</a>] <br><br> <strong>Reading guide</strong><br> So far, we have looked into monolithic inference procedures. We have also seen that we have so many of them because there is no perfect one. In this lecture, we focus on techniques that make it possible to utilise the strengths of several inference procedures for the same problem. When reading these papers, focus on the new aspects they bring and the aspects that could not be done with inference procedures we looked ata so far. Try to understand the theorems, but don’t bother with proofs. <br><br> <strong>If you want to know more</strong> <br> <a href="https://arxiv.org/pdf/2103.00668.pdf" rel="external nofollow noopener" target="_blank">Learning Proposals for Probabilistic Programs with Inference Combinators</a> <br> <a href="https://dl.acm.org/doi/pdf/10.1145/3314221.3314642" rel="external nofollow noopener" target="_blank">Gen: a general-purpose probabilistic programming system with programmable inference</a> <br> <a href="https://arxiv.org/pdf/1404.0099.pdf" rel="external nofollow noopener" target="_blank">Venture: a higher-order probabilistic programming platform with programmable inference</a> </td> </tr> <tr> <td>W5 L10 <br> October 6 <br><br> Martin <br> Marije</td> <td style="text-align: left"> <strong>Topic</strong><br> Simulation-based inference <br><br> <strong>Required literature</strong> <br> <a href="https://proceedings.neurips.cc/paper/2019/file/6d19c113404cee55b4036fce1a37c058-Paper.pdf" rel="external nofollow noopener" target="_blank">Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model</a> [<a href="#w5-l10-p1">Review form</a>] <br> <a href="https://arxiv.org/pdf/2003.13221.pdf" rel="external nofollow noopener" target="_blank">Planning as Inference in Epidemiological Models</a> [<a href="#w5-l10-p2">Review form</a>] <br><br> <strong>Reading guide</strong><br> When reading these papers focus on (1) understanding why probabilistic programming is a suitable paradigm for this task, (2) which kind of inference techniques were used, and (3) what are the challenges in applying probabilistic programming to simulators. For the second paper on epidemiological models, you don’t have to read the entire paper; use your judgement to identify the important parts. <br><br> <strong>If you want to know more</strong><br> <a href="https://arxiv.org/pdf/1911.01429.pdf" rel="external nofollow noopener" target="_blank">The frontier of simulation-based inference</a> <br> <a href="http://physadept.csail.mit.edu/papers/adept.pdf" rel="external nofollow noopener" target="_blank">Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations</a> <br> <a href="https://par.nsf.gov/servlets/purl/10169392" rel="external nofollow noopener" target="_blank">Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale</a> <br> <a href="https://arxiv.org/pdf/2112.03235.pdf" rel="external nofollow noopener" target="_blank">Simulation Intelligence: Towards a New Generation of Scientific Methods</a> <br> <a href="http://proceedings.mlr.press/v108/warrington20a/warrington20a.pdf" rel="external nofollow noopener" target="_blank">Coping With Simulators That Don’t Always Return</a> <br> <a href="https://arxiv.org/pdf/1910.11950.pdf" rel="external nofollow noopener" target="_blank">Probabilistic Surrogate Networks for Simulators with Unbounded Randomness</a> </td> </tr> <tr> <td>W6 L11 <br> October 10 <br><br> Sneha <br> Reuben</td> <td style="text-align: left"> <strong>Topic</strong><br> How to efficiently handle traces with stochastic support? <br><br> <strong>Required literature</strong><br><a href="https://arxiv.org/pdf/2007.09871.pdf" rel="external nofollow noopener" target="_blank">Automating Involutive MCMC using Probabilistic and Differentiable Programming</a> [<a href="#w6-l11-p1">Review form</a>] <br> <a href="https://arxiv.org/pdf/1910.13324.pdf" rel="external nofollow noopener" target="_blank">Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic Programs with Stochastic Support</a> [<a href="#w6-l11-p2">Review form</a>] <br><br> <strong>Reading guide</strong><br> Focus on the key idea how these two technqiues make it possible to easily switch between traces with different number of stochastic choices. Skip, or skim, the measure theoretic stuff in the first paper.</td> </tr> <tr> <td>W6 L12 <br> October 13 <br><br> Dāvis</td> <td style="text-align: left"> <strong>Topic</strong><br> Alternative paradigms of probabilistic programming – probabilistic logic programming <br><br> <strong>Required literature</strong><br> <a href="https://dtai.cs.kuleuven.be/publications/files/42447.pdf" rel="external nofollow noopener" target="_blank">ProbLog: A Probabilistic Prolog and its Application in Link Discovery</a> [<a href="#w6-l12-p1">Review form</a>] <br> <a href="https://lirias.kuleuven.be/retrieve/190816" rel="external nofollow noopener" target="_blank">k-Optimal: A novel approximate inference algorithm for Problog</a> [<a href="#w6-l12-p2">Review form</a>]<br><br> <strong>Reading guide</strong><br> In the next two lectures, we will explore some alternative paradigm in probabilistic programming. You should focus on what do these new paradigm do differently than the techniques we have seen so far, what are the assumptions and deisign choices they make. Reading the <a href="https://book.simply-logical.space/src/text/1_part_i/3.0.html" rel="external nofollow noopener" target="_blank">Section 3 and 3.1 from simply Logical</a> before will make the papers more understandable. What do these paradigm bring to the probabilistic programming scene? <br><br><strong>If you want to know more</strong><br> <a href="https://arxiv.org/pdf/1202.3719.pdf" rel="external nofollow noopener" target="_blank">Inference in Probabilistic Logic Programs using Weighted CNFs</a> <br> <a href="https://lirias.kuleuven.be/bitstream/123456789/490338/1/deraedt_kimmig_mlj15.pdf" rel="external nofollow noopener" target="_blank">Probabilistic (Logic) Programming Concepts</a> <br> <a href="https://lirias.kuleuven.be/bitstream/123456789/411248/1/mcmc.pdf" rel="external nofollow noopener" target="_blank">MCMC Estimation of Conditional Probabilities in Probabilistic Programming Languages</a> <br> <a href="https://lirias.kuleuven.be/retrieve/550383/" rel="external nofollow noopener" target="_blank">DeepProbLog: Neural Probabilistic Logic Programming</a> <br> <a href="https://rjida.meijo-u.ac.jp/reference/Sato-ICLP09.pdf" rel="external nofollow noopener" target="_blank">Generative modeling by PRISM</a> <br> <a href="https://arxiv.org/pdf/1301.3846.pdf#:~:text=A%20stochastic%20logic%20program%20(SLP,1996%3B%20Cussens%2C%201999)." rel="external nofollow noopener" target="_blank">Stochastic Logic Programs: Sampling, Inference and Applications</a> <br> <a href="https://people.eecs.berkeley.edu/~russell/papers/srlbook-blog.pdf" rel="external nofollow noopener" target="_blank">BLOG: Probabilsitic Models with Unknown Objects</a> <br> <a href="https://files.sri.inf.ethz.ch/website/papers/psi-solver.pdf" rel="external nofollow noopener" target="_blank">PSI: Exact Symbolic Inference for Probabilistic Programs</a> <br> <a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.360.1462&amp;rep=rep1&amp;type=pdf" rel="external nofollow noopener" target="_blank">Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths</a> <br> <a href="https://stuhlmueller.org/papers/nonstandard-interpretations-nips2011.pdf" rel="external nofollow noopener" target="_blank">Nonstandard Interpretations of Probabilistic Programs for Efficient Inference</a> </td> </tr> <tr> <td>W7 L13 <br> October 17<br><br> Sara <br> Raphaël</td> <td style="text-align: left"> <strong>Topic</strong><br> Alternating paradigms of probabilistic programming – Deep generative models <br><br> <strong>Required readings</strong><br> <a href="https://arxiv.org/pdf/1606.05908.pdf" rel="external nofollow noopener" target="_blank">Variational Autoencoders</a> [<a href="#w7-l13-p1">Review form</a>] <br> <a href="https://arxiv.org/pdf/1505.05770.pdf" rel="external nofollow noopener" target="_blank">Variational Inference with Normalising Flows</a> [<a href="#w7-l13-p2">Review form</a>] <br><br> <strong>Reading guide</strong><br> Deep generative models are not technically probabilistic programming paradigm, but it follows some of the same idea – they focus on modelling generative processes. When reading these papers, focus on understanding a different perspective these approaches take towards generative modelling, how is probability interpreted, and what can they do that probabilistic programs can’t (and vice versa!).. <br><br> <strong>If you want to know more</strong><br> <a href="https://link.springer.com/book/10.1007/978-3-030-93158-2" rel="external nofollow noopener" target="_blank">Deep Generative Modelling</a> <br> <a href="https://jmlr.org/papers/volume22/19-1028/19-1028.pdf" rel="external nofollow noopener" target="_blank">Normalizing Flows for Probabilistic Modeling and Inference</a> <br> <a href="https://proceedings.neurips.cc/paper/2018/file/201e5bacd665709851b77148e225b332-Paper.pdf" rel="external nofollow noopener" target="_blank">Simple, Distributed, and Accelerated Probabilistic Programming</a> </td> </tr> <tr> <td>W7 L14 <br> October 20 <br><br> Guest lecture by <a href="https://www.khoury.northeastern.edu/home/sholtzen/" rel="external nofollow noopener" target="_blank">Steven Holtzen</a> </td> <td style="text-align: left"> <strong>Topic</strong><br> Discrete Probabilistic Programming <br><br> <strong>Required literature</strong><br> <a href="https://www.sciencedirect.com/science/article/pii/S0004370207001889" rel="external nofollow noopener" target="_blank">On probabilistic inference by weighted model counting</a> [<a href="#w7-l14-p1">Review form</a>] <br> <strong>Lecture will be based on this paper:</strong> <br> <a href="https://arxiv.org/pdf/2005.09089.pdf" rel="external nofollow noopener" target="_blank">Scaling Exact Inference for Discrete Probabilistic Programs</a> <br><br> <strong>Reading guide</strong><br>This week you will read more about weighted model counting, a technique we have already touched upon with Problog. WMC is also a technique on which today’s guest lecture is heavily based on. You don’t have to read the entire paper for the lecture; it is quite a long one. What you should aim to understand what the technique is and how it is used to perform probabilistic inference. you don’t have to understand the construction algorithms and other things. <br> <br><strong>If you want to know more</strong><br><a href="https://arxiv.org/pdf/1211.4475.pdf" rel="external nofollow noopener" target="_blank">Algebraic Model Counting</a> <br> <a href="https://arxiv.org/pdf/2010.03485.pdf" rel="external nofollow noopener" target="_blank">SPPL: Probabilistic Programming with Fast Exact Symbolic Inference</a> </td> </tr> <tr> <td>W8 L15 <br> October 24 <br><br> Cristian <br> Balazs <br> Farhad</td> <td style="text-align: left"> <strong>Topic</strong><br> Cool applications and powerful inference procedures <br><br> <strong>Required literature</strong><br><a href="https://arxiv.org/pdf/2111.00312.pdf" rel="external nofollow noopener" target="_blank">3DP3: 3D Scene Perception via Probabilistic Programming</a> <a href="#w8-l15-p1">[Review form]</a> <br> <a href="https://arxiv.org/pdf/2005.14062.pdf" rel="external nofollow noopener" target="_blank">Inferring Signaling Pathways with Probabilistic Programming</a> <a href="#w8-l15-p2">[Review form]</a> <br> <a href="http://proceedings.mlr.press/v139/tavares21a/tavares21a.pdf" rel="external nofollow noopener" target="_blank">A Language for Counterfactual Generative Models</a> <br><br> <strong>Reading guide</strong><br> When reading these papers, focus on what does every element of a probabilistic program represents (e.g., priors, likelihood, posteriors, observations). It is not necessary that you understand all the math. <br><br> <strong>If you want to know more:</strong> <br> <a href="https://arxiv.org/pdf/1907.03382.pdf" rel="external nofollow noopener" target="_blank">Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale</a> <br> <a href="https://openreview.net/pdf?id=rJgHCgc6pX" rel="external nofollow noopener" target="_blank">Alexandria: Unsupervised High-Precision Knowledge Base Construction using a Probabilistic Program</a> <br> <a href="https://arxiv.org/pdf/1606.07046.pdf" rel="external nofollow noopener" target="_blank">Semantic Parsing to Probabilistic Programs for Situated Question Answering</a> <br> <a href="https://arxiv.org/pdf/2007.11838.pdf" rel="external nofollow noopener" target="_blank">PClean: Bayesian Data Cleaning at Scale with Domain-Specific Probabilistic Programming</a> </td> </tr> <tr> <td>W8 L16 <br> October 27 <br><br> Guest lecture by <a href="https://www.robots.ox.ac.uk/~twgr/" rel="external nofollow noopener" target="_blank">Tom Rainforth</a> </td> <td style="text-align: left"> <strong>Nested Probabilistic Programs and Target Aware Inference and Expectation Programming</strong> <br> We formalize the notion of nesting probabilistic programming queries and investigate the resulting statistical implications. We demonstrate that while query nesting allows the definition of models which could not otherwise be expressed, such as those involving agents reasoning about other agents, existing systems take approaches which lead to inconsistent estimates. We show how to correct this by delineating possible ways one might want to nest queries and asserting the respective conditions required for convergence. We further introduce a new online nested Monte Carlo estimator that makes it substantially easier to ensure these conditions are met, thereby providing a simple framework for designing statistically correct inference engines. We prove the correctness of this online estimator and show that, when using the recommended setup, its asymptotic variance is always better than that of the equivalent fixed estimator, while its bias is always within a factor of two <br><br> <strong>Target Aware Inference and Expectation Programming</strong> <br> Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions—a computational pipeline that is inefficient when the target function(s) are known upfront. We address this inefficiency by introducing a framework for target–aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard (1/N) Monte Carlo rate, potentially producing rates as fast as (1/N2). We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target-aware inference scheme and demonstrate the substantial benefits this can yield.</td> </tr> </tbody> </table> <h2 id="other-materials">Other materials</h2> <h3 id="additional-materials">Additional materials</h3> <p>The course does not have a required textbook, it instead relies on selected papers. Some materials that might help you better understand probabilistic programming:</p> <ul> <li> <a href="https://probmods.org/index.html" rel="external nofollow noopener" target="_blank">Probabilistic Models of Cognition</a>: the book uses probabilistic programs to formulate models of human cognition. The book is a great introduction to using probabilistic programming and contains plenty of example to practice your modelling skills.</li> <li> <a href="http://dippl.org/" rel="external nofollow noopener" target="_blank">The Design and Implementation of Probabilistic Programming Languages</a>: a high-level book covering the basic inference procedures.</li> <li> <a href="https://arxiv.org/pdf/1809.10756.pdf" rel="external nofollow noopener" target="_blank">An Introduction to Probabilistic Programming</a>: a work-in-progress introductory textbook. Covers many topics of the course. Very good introduction to basic inference procedures, implementation strategies, and variational inference.</li> <li> <a href="https://www.morganclaypool.com/doi/10.2200/S00692ED1V01Y201601AIM032" rel="external nofollow noopener" target="_blank">Statistical Relational Artificial Intelligence: Logic, Probability, and Computation</a>: a high-level introduction to various paradigms of probabilistic logic programming.</li> <li> <a href="http://mcs.unife.it/~friguzzi/plp-book.html" rel="external nofollow noopener" target="_blank">Foundations of Probabilistic Logic Programming</a>: a deeper introduction to probabilistic logic programming.</li> <li> <a href="https://www.cis.upenn.edu/~bcpierce/tapl/" rel="external nofollow noopener" target="_blank">Types and programming languages</a>: good introduction to foundations of programming languages.</li> <li> <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" rel="external nofollow noopener" target="_blank">Probabilistic Programming and Bayesian Methods for Hackers</a>: an easy to follow book, great at explaining key ideas intuitively.</li> </ul> <h3 id="additional-topics">Additional topics</h3> <p>Probabilistic programming is rapidly developing field. There are many topics we will not be able to cover in the class. Here is a snapshot of such topics, if you want to know more:</p> <ul> <li>Other inference tasks: The inference task we look at was the characterisation of the posterior distribution. What if we are interested in other tasks, e.g., maximising the posterior? <ul> <li><a href="https://proceedings.neurips.cc/paper/2016/file/31fefc0e570cb3860f2a6d4b38c6490d-Paper.pdf" rel="external nofollow noopener" target="_blank">Bayesian Optimization for Probabilistic Programs</a></li> <li><a href="https://arxiv.org/pdf/2106.04953.pdf" rel="external nofollow noopener" target="_blank">Expectation Programming: Adapting Probabilistic Programming Systems to Estimate Expectations Efficiently</a></li> </ul> </li> <li>Learning probabilsitic programs: a standard PP pipeline focuses on inference; but what if we don’t know the program? <ul> <li><a href="https://www.science.org/doi/epdf/10.1126/science.aab3050" rel="external nofollow noopener" target="_blank">Human-level concept learning through probabilistic program induction</a></li> <li><a href="https://arxiv.org/pdf/1907.06249.pdf" rel="external nofollow noopener" target="_blank">Bayesian Synthesis of Probabilistic Programs for Automatic Data Modeling</a></li> <li><a href="https://arxiv.org/pdf/1703.05698.pdf" rel="external nofollow noopener" target="_blank">Neural Sketch Learning for Conditional Program Generation</a></li> </ul> </li> <li>Semantics of probabilistic programming languages: how to define the meaning of a probabilistic program? <ul> <li><a href="https://www.cambridge.org/core/books/foundations-of-probabilistic-programming/semantics-of-probabilistic-programming-a-gentle-introduction/A7964205E44B5234A78C661192E294E1" rel="external nofollow noopener" target="_blank">Semantics of Probabilistic Programming: A Gentle Introduction</a></li> <li><a href="https://arxiv.org/pdf/1601.04943.pdf" rel="external nofollow noopener" target="_blank">Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints</a></li> <li><a href="https://www.cs.cornell.edu/~kozen/Papers/ProbSem.pdf" rel="external nofollow noopener" target="_blank">Semantics of Probabilistic programs</a></li> </ul> </li> <li>Deep generative models: an alternative framework for generative modelling based on neural networks. It has less flexibility than PPL but better learning properties. <ul> <li><a href="https://link.springer.com/book/10.1007/978-3-030-93158-2" rel="external nofollow noopener" target="_blank">Deep Generative Modelling</a></li> </ul> </li> <li>Analysis of probabilistic programs: <ul> <li><a href="https://arxiv.org/pdf/2010.11887.pdf" rel="external nofollow noopener" target="_blank">Conditional independence by typing</a></li> <li><a href="https://dl.acm.org/doi/pdf/10.1145/2676726.2677001" rel="external nofollow noopener" target="_blank">Probabilistic Termination: Soundness, Completeness, and Compositionality</a></li> </ul> </li> <li>Nested probabilistic programs: probabilistic programs that call probabilistic programs <ul> <li><a href="https://arxiv.org/pdf/1803.06328.pdf" rel="external nofollow noopener" target="_blank">Nesting probabilistic programs</a></li> <li><a href="https://www.sciencedirect.com/science/article/pii/S1389041713000387" rel="external nofollow noopener" target="_blank">Reasoning about reasoning by nested conditioning: Modeling theory of mind with probabilistic programs</a></li> <li><a href="https://arxiv.org/pdf/1612.00951.pdf" rel="external nofollow noopener" target="_blank">On the Pitfalls of Nested Monte Carlo</a></li> </ul> </li> <li>Stochastic conditioning: in the course, the observations always came in form of the exact value of a variable. What if we want to condition on a distribution? <ul> <li><a href="https://arxiv.org/pdf/2010.00282.pdf" rel="external nofollow noopener" target="_blank">Probabilistic Programs with Stochastic Conditioning</a></li> </ul> </li> <li>Deep probabilistic programming: we have touched upon various interaction between probabilistic programming and deep neural networks. Deep probabilistic programming is a paradigm that combines the two, often with murky boundaries from e.g. variational or amortised inference. <ul> <li><a href="https://www.sciencedirect.com/science/article/pii/S0004370221000552" rel="external nofollow noopener" target="_blank">Neural probabilistic logic programming</a></li> <li><a href="https://arxiv.org/abs/1701.03757?context=cs.PL" rel="external nofollow noopener" target="_blank">Deep Probabilistic Programming</a></li> </ul> </li> </ul> <h3 id="courses-at-other-universities">Courses at other universities</h3> <ul> <li> <a href="https://www.cs.ubc.ca/~fwood/CS532W-539W/" rel="external nofollow noopener" target="_blank">Probabilistic programming</a> by Frank Wood at University of British Columbia, Canada</li> <li> <a href="https://www.khoury.northeastern.edu/home/sholtzen/CS7480Fall21/" rel="external nofollow noopener" target="_blank">seminar course</a> by Steven Holtzen at Northeastern, USA</li> <li> <a href="https://docs.google.com/document/d/1MJXs0OdUS9sVL3LjTfEFK6i-Jz06cAKG21N_auE85wM/pub" rel="external nofollow noopener" target="_blank">seminar course</a> by Dan Roy at University of Toronto, Canada, which focuses on theoretical aspects of probabilistic programming</li> <li> <a href="https://github.com/aleatory-science/deep-probprog-course" rel="external nofollow noopener" target="_blank">Deep Probabilistic Programming course</a> by Thomas Hamelryck and Ahmad Salim Al-Sibahi at University of Copenhagen, Denmark</li> <li> <a href="https://www.cs.tufts.edu/comp/150PP/" rel="external nofollow noopener" target="_blank">Probabilistic Programming Languages course</a> by Norman Ramsey at Tufts University, USA</li> <li> <a href="http://web.cs.ucla.edu/~guyvdb/teaching/cs267a/2022s/" rel="external nofollow noopener" target="_blank">Probabilistic Programming and Relational Learning</a> bu Guy Van den Broeck at UCLA, USA.</li> <li> <a href="https://github.com/hongseok-yang/probprog17" rel="external nofollow noopener" target="_blank">Probabilistic Programming course</a> by Hongseok Yang at KAIST, South Korea</li> </ul> <h2 id="practicalities">Practicalities</h2> <h3 id="how-to-do-well-in-a-seminar-course">How to do well in a seminar course</h3> <p>A seminar course is different from a standard course in two ways: (1) “just” learning the topics covered by the materials is not enough; and (2) you are expected to actively participate in the lectures. The active participation includes giving a presentation (see <a href="#preparing-you-presentation">Preparing your presentation</a>) and discussing the materials during the lecture. Actually, the lectures will consist only of these two activities. The exceptions are week 1, which contain two lectures by the instructor, and week 2, in which we discuss papers but no one has to prepare the presentation. Each student will present one paper and is expected to actively contribute in discussions.</p> <p>If understanding the materials is not enough, what is? In a seminar course, we teach you to think beyond what you have been served. In fact, if you only master the materials, that will get you a mere 6. Instead, you are expected to critically analyse every paper you read. For every paper you read, think about the following questions:</p> <ul> <li>What is the main idea proposed in the paper?</li> <li>Which problem does it solve?</li> <li>What are its strengths?</li> <li>What are its weaknesses?</li> <li>Which parts of the paper were/and perhaps still are confusing to you?</li> <li>Do you have an impression that the idea works just because the authors made some particular choices?</li> <li>What are the implicit assumption the authors are making?</li> <li>Do you see multiple options to solve a subproblem X and are wondering why the authors chose</li> <li>Is the idea properly evaluated? If not, what is missing? How would you do it?</li> <li>Are the arguments for the idea convincing?</li> <li>How would you extend the work?</li> </ul> <p>During the discussion part of the lectures, we will discuss these questions and compare your observation. Not all questions are applicable to every paper, so don’t worry if you cannot relate some of them to some papers. The quality of the discussion will therefore depend on you. Importantly, you have to come prepared for each lecture.</p> <p>It is important that you actively participate in the discussion. As a rule of thumb, the more you talk, the higher your grade will be in the end. However, <em>be constructive</em> and avoid saying something just to say something. I don’t want you to be quiet the entire quarter, but that does not mean that you have to say something in every discussion. Sometimes you would have something brief to say, sometimes you would have some deeper insight. That is fine. As a rule of thumb, I expect you to say something at least every few lectures. Don’t be upset if I respond to you or even correct you. For most of you, this is the first time you are participating in a seminar. It is expected that you need some guidance. Moreover, I tend to jump in when you mention something interesting without (perhaps) realising it.</p> <p>An important difference between a regular course and a seminar is that you will not be penalised for not understanding something. That does not mean that this is a free-pass course, but rather that it is ok to build up towards an understnading during the course. You will be reading research papers, often in depth.<br> In contrast to textbooks, research papers are concise and expect certain knowledge from a reader. This has two consequence. First, it is likely that you will not know some of the necessary concepts and would have to fill the gap to understand the paper. You should go through that process. Second, scientists are often not great writers – they might explain simple concepts very confusingly because they have different audience in mind (peers, not students). For any of these reasons, you might not understand a part of the paper. This is normal and you should not hide it – confusing parts of the paper are great starters for discussions!</p> <p>Here are a few situations that are acceptable in a seminar course, but perhaps not in a regular course:</p> <ol> <li>You misunderstand something in a paper and provide a wrong answer in a review. If you realise that during the lecture, you can correct you answer after the lecture. That is fine.</li> <li>You misunderstand something that leads you to an interesting idea for an extension. This is perfectly fine if your reasoning taking the misunderstanding as a fact is correct and interesting.</li> <li>You just can’t understand what the paper is trying to say. Voice it in the lecture! Try to phrase it as a detailed question; point out as specifically as possible where you lost it</li> </ol> <h3 id="grading-scheme">Grading scheme</h3> <p>The tables below lists the expected competences for passing grades. the descriptions are cummulative; e.g., expectations for the grade 8 also include expectations for grades 6 and 7.</p> <p><strong>Project</strong></p> <style scoped="">table{font-size:13px}</style> <table> <thead> <tr> <th><strong>Grade</strong></th> <th><strong>Competences that need to be demonstrated</strong></th> </tr> </thead> <tbody> <tr> <td>6</td> <td>Running a large number of off-the-shelf inference procedures on several problems</td> </tr> <tr> <td>7</td> <td>Composition of basic inference procedures, with minor modifications to their original form</td> </tr> <tr> <td>8</td> <td>Development of problem-specific inference procedures that leverage problem structure and domain insights in a non-trivial way <br> Basic procedure that generalises across problems <br> Contribution in terms of new problems or innovative rewrites of provided programs</td> </tr> <tr> <td>9</td> <td>Use of deep theoretical insights <br> Development of a single inference procedure that works well on different problems</td> </tr> <tr> <td>10</td> <td>Execution that can be published as a research paper <br> Using insights beyond what was explicitly covered in the lectures</td> </tr> </tbody> </table> <p><strong>Presentation, discussions, reviews</strong></p> <table> <thead> <tr> <th><strong>Grade</strong></th> <th><strong>Competences that need to be demonstrated</strong></th> </tr> </thead> <tbody> <tr> <td>6</td> <td>Understandable presentation, but no effort to make the topic more understandable through materials beyond the provided reading <br><br> The presentation and reviews demonstrate shallow understanding of most of the topics <br><br> Critical analysis is shallow <br><br> Active participation only in few discussions</td> </tr> <tr> <td>7</td> <td>Presentation make extensive use of visualisations and examples <br><br>The presentation and reviews demonstrate good understanding of the topics and very good understanding of a few of them <br><br> Critical analysis is focused on the limitations outlined in a paper, but a student can elaborate in more details why <br><br> Frequent participation in discussions, but not going too much into depth</td> </tr> <tr> <td>8</td> <td>Presentation draws connections to other topics covered in the course <br><br> The presentations and reviews demonstrate very good understanding of the topics <br><br> Provide critical analysis beyond what is outlined in the papers while also identifying solutions <br><br> Frequent participation in discussions, with a deeper insight demonstrated in few discussions</td> </tr> <tr> <td>9</td> <td>Presentation offers a very comprehensive introduction to the topic, broader than the required reading <br><br> Reviews demonstrate a very good understanding of all topics <br><br> Critical analysis goes beyond the limitations in the paper, the weaknesses are well elaborated, and the analysis contains concrete solutions <br><br> Active participation in the majority of discussions, with frequently sharing deeper insights about the topics</td> </tr> <tr> <td>10</td> <td>Demonstrable understanding of the topics beyond what was required. <br><br> The equivalent of identifying a research problem and conceiving a detailed research plan how to address it</td> </tr> </tbody> </table> <h3 id="preparing-your-presentation">Preparing your presentation</h3> <p>You will have to present one paper to the group. Student presentation start in week 3.</p> <p>Your presentation should take between 15 and 20 mins. This is an estimate, some papers might require less while other more time. If you think you have to go beyond this limit, discuss it with me.</p> <p>Approach your presentation as a short lecture. You goal is to explain the topic to your colleagues so that they learn from the lecture; only half of the student in the class will have read the paper. Think about the right way to explain the topic; this might not be the way it was explained in the paper. For instance, if you found that you had to look a lot of other concepts, explain them before the topic you are covering. Use illustrations and animations, you can find lots of them online (if not for the exact topic, then for something very related that could help you explain the intuition). Provide a working example, especially if such an example is not present in the original paper.</p> <p>Feel free to use “non-academic” materials for your preparations: blogs, videos, informal notes… I don’t care what you use as long as you understand the topic. Importantly, if you encounter materials that you found more useful than the one I proposed, share them with me.</p> <p>Your main focus in the presentation should be to convey the idea/concept to your colleagues. This usually means that you have to think about not only how to convey the idea effectively but also which parts of the paper to include or skip. There will be no correlation between the amount of content squeezed in a lecture and a grade. Often it makes sense to skip something to optimise for clarity.</p> <p><strong>Important:</strong> schedule a meeting with me at least 2 days before your presentation; this is to ensure that your presentation is of sufficient quality to provide a valuable learning experience to your colleagues.</p> <h3 id="project-rules">Project rules</h3> <p>The project has few rules:</p> <ul> <li>you can use any PPL you want. I would encourage you to play with multiple ones. Choose the tools based on what they can do for you rather than which language are they implemented in. I would strongly suggest to at least have a look at the PPLs that support programmable inference (such as Gen, Turing, and Pyro). Some tools you might consider: <a href="https://www.gen.dev/" rel="external nofollow noopener" target="_blank">Gen.jl</a>, <a href="https://pyro.ai/" rel="external nofollow noopener" target="_blank">Pyro</a>, <a href="https://docs.pymc.io/en/v3/index.html" rel="external nofollow noopener" target="_blank">PyMC3</a>, <a href="https://probprog.github.io/anglican/" rel="external nofollow noopener" target="_blank">Anglican</a>, <a href="https://turing.ml/stable/" rel="external nofollow noopener" target="_blank">Turing.jl</a>, <a href="https://mc-stan.org/" rel="external nofollow noopener" target="_blank">Stan</a>, <a href="https://github.com/probtorch/probtorch" rel="external nofollow noopener" target="_blank">ProbTorch</a>, <a href="https://www.tensorflow.org/probability" rel="external nofollow noopener" target="_blank">TensorFlow probability</a>, <a href="https://beanmachine.org/" rel="external nofollow noopener" target="_blank">Bean Machine</a>.</li> <li>Form teams of 2 or 3</li> <li>Your team can submit floor(N/2) inference procedures, where N is the number of problems. All submitted procedures will be run on every problem, and the best result will be kept for each problem. The idea behind limiting this is to force you to develop at least some inference procedures that work on multiple problems. You can decide how to divide the inference procedures over tasks: you can made a few submissions that specialise on a particular problem and accompany it with one general inference procedures, or vice versa. You general inference procedures can be <em>meta-procedures</em>: they can try to figure out which inference procedure to use (or their composition) heuristically.</li> <li>You can make as many submissions as possible. We will run the evaluation once per week with the most recent submission. Every submission needs to be followed with a log entry (i.e., <a href="#project-reports">project report</a>).</li> <li>You can submit your own problems. Once I approve your problem, all teams need to consider them.</li> <li>You have to use a common Docker image.</li> <li>You can re-write the given programs, as long as they retain the same meaning.</li> <li>createa Github repository and share it with me.</li> </ul> <h3 id="project-reports">Project reports</h3> <p>You will not be making a standard project report that you submit once at the end of the quarter summarising your findings. Instead, the project report will be the logs that you make with each project submission. The logs have free form but have to answer two questions: (1) what did you change, and (2) why did you make that change. The ansers to these questions do not have to be extensive elaborations, but should be a concise summary sufficient for me to understand your motivations.</p> <p>Don’t interpret ‘motivation’ too strictly. It is acceptable to occasionally just try out something. You just don’t want you rreport to be a collection of random tryouts. It is also ok to have a huntch which you cannot fully ground in the theory explored during the lectures.</p> <p>Don’t be shy with negative results. Sometimes you have an idea you think would be successful, but is not. This is normal. This is good progress if your motivation for trying it out makes sense. Note that ‘makes sense’ is different from ‘correct’. You might have made a mistake in your reasoning and that is also a part of the process (this can still earn you points).</p> <p>Upload your log in Brightspace (Project). The log of each submission should be a separate file.</p> <h3 id="course-feedback">Course feedback</h3> <p>This is the first edition of the course. As you can imagine, this is kind of a beta version of the course. To make sure the course offers the best learning experience to you, I would appreciate if you provide feedback throughout the course. What works for you? What doesn’t? Do you have an idea how to improve something?</p> <p>You can submit the feedback anonymously <a href="https://forms.gle/PUGRCUwsZp4HNGbC7" rel="external nofollow noopener" target="_blank">HERE</a>.</p> <h3 id="paper-reviews">Paper reviews</h3> <h4 id="w3-l5-p1">W3 L5 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Apply the transformation to the if-then statement starting at line 9 in Figure 1</p> </li> <li> <p>Why focus on weakest precondition analysis? Wouldn’t it make sense to derive stronger conditions?</p> </li> <li> <p>Give an example of a potential weakest precondition rule for continuous case.</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w3-l5-p2">W3 L5 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>How does exactly non-centering make sampling more efficient?</p> </li> <li> <p>Work out an implementation of the non-centering transformation in one of the paradigms of L4.</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w3-l6-p2">W3 L6 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why is Hamiltonian dynamics interesting for probabilistic inference? What does it buy us?</p> </li> <li> <p>Take a closer look at equation 11.67. What does the difference in two Hamiltonians actually capture?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w3-l6-p1">W3 L6 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What is the main difference between forward and reverse mode autmamted differentiation?</p> </li> <li> <p>When is forward (reverse) mode more efficient?</p> </li> <li> <p>How is it possible that we can differentiate an entire program, with complex control flow and branching?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w4-l7-p1">W4 L7 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the idea behind variational inference in your own words.</p> </li> <li> <p>How does this work create a variational equivalent of the provided program?</p> </li> <li> <p>What are the conditions the target program needs to fulfil in order to use variational inference (of the kind described in the paper)?</p> </li> <li> <p>When will variational inference fail?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w4-l7-p2">W4 L7 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the idea behind variational inference in your own words.</p> </li> <li> <p>When will variational inference fail?</p> </li> <li> <p>Does this technique works for any probabilistic program?</p> </li> <li> <p>Why are the two transformations necessary?</p> </li> <li> <p>Which part of the framework is variationally approximated?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w4-l8-p1">W4 L8 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What is neural networks predicting exactly?</p> </li> <li> <p>What is the difference compared to variational inference?</p> </li> <li> <p>The paper pays special attention to gradient estimators. The variational inference paper we looked into did not do that. Why is this issue important for this paper?</p> </li> <li> <p>How does the paper make discrete probabilistic choices ‘differentiable’?</p> </li> <li> <p>Why is the function mapData important in this work?</p> </li> <li> <p>How does the paper ensure that the guide program keeps the control flow of the starting probabilistic program?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w4-l8-p2">W4 L8 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What is neural networks predicting exactly?</p> </li> <li> <p>What is the difference compared to variational inference?</p> </li> <li> <p>Why did the authors chose the LSTM to model a program?</p> </li> <li> <p>Assume you have trained a compiled inference network, but your query suddenly changes together with the observations. Can you reuse your compiled inference network?</p> </li> <li> <p>Could you use this idea in other inference algorithms, beyond importance sampling? If yes, how?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w5-l9-p1">W5 L9 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What is the key computational abstraction that makes programmable inference possible?</p> </li> <li> <p>Can an inference procedure over a subproblem change any part of a program?</p> </li> <li> <p>How does Venture ensures that the trace remains valid after an inference step for a subproblem?</p> </li> <li> <p>What is a valid trace?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w5-l9-p2">W5 L9 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Give an example of a problem that could be difficult to solve with <em>standard</em> proposal but easy to do with a specific programmatic proposal? Give a sketch of such a proposal program?</p> </li> <li> <p>what is the difference between <em>output choices</em> and <em>internal choices</em>?</p> </li> <li> <p>Explain in your words why programs as proposal do not affect the guarantees offered by Importance sampling and Metropolis-Hastings inference algorithm?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w5-l10-p1">W5 L10 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why is the consider problem suitable for probabilistic programming?</p> </li> <li> <p>Which quantities of the considered model are uncertain/probabilitic?</p> </li> <li> <p>What is a big challenge in applying probabilistic inference to physics problems?</p> </li> <li> <p>Do inference procedures we hve seen in the course require any modification to be applied to this problem? Why?</p> </li> <li> <p>What needs to be changed in the original simulator to use it within probabilistic programming engine?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w5-l10-p2">W5 L10 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why is the consider problem suitable for probabilistic programming?</p> </li> <li> <p>What are the assumptions that the paper makes?</p> </li> <li> <p>What are the sources of uncertanty in epidemiological models?</p> </li> <li> <p>Do inference procedures we hve seen in the course require any modification to be applied to this problem? Why?</p> </li> <li> <p>What needs to be changed in the original simulator to use it within probabilistic programming engine?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w6-l11-p1">W6 L11 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Involutive MCMC needs three ingredients? Which ones and why are they needed?</p> </li> <li> <p>What does the auxiliary variables do? Try to relate it to other inference procedure we have seen in the class?</p> </li> <li> <p>What are the key abstractions that this paper contributes in order to make involutive MCMC applicable to any problem that can be expressed in probabilistic programming?</p> </li> <li> <p>What is the key part that makes it possible to easily switch between traces with different number of stochastic choices?</p> </li> <li> <p>Probabilistic programming perspective makes one part of involutive MCMC easy. Which part is that?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w6-l11-p2">W6 L11 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why does the work focuses on straight-line programs?</p> </li> <li> <p>Why can we so easily combine samples of individual straight-line programs?</p> </li> <li> <p>What is the role of resource allocation in DCC?</p> </li> <li> <p>Explain in your words why inference procedures likw Metropolis-Hastings MCMC and paricle filtering are bad at problems with stochastic support?</p> </li> <li> <p>DCC draws samples from two <em>spaces</em>; which ones and why are they needed?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w6-l12-p1">W6 L12 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why is the combination of logic and probability interesting?</p> </li> <li> <p>What is the role of weighted model counting in Problog?</p> </li> <li> <p>What is the advantage of Problog when it comes to finding executions of a program that lead to the query?</p> </li> <li> <p>Why is a disjoint sum a problem?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w6-l12-p2">W6 L12 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>Why is the combination of logic and probability interesting?</p> </li> <li> <p>What makes a logical proof more important?</p> </li> <li> <p>What is the main difference between this kind of approximation and other approximate techniques we have seen in the course?</p> </li> <li> <p>How do we find the optimal proofs efficiently?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution</p> </li> </ul> <h4 id="w7-l13-p1">W7 L13 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What makes VAE variational?</p> </li> <li> <p>What issue does the reparametrization trick solve?</p> </li> <li> <p>VAE does not make any assumption about the structure of data, unlike a probabilistic program. Explain what that means?</p> </li> <li> <p>What can a probabilistic program do that VAE cannot?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w7-l13-p2">W7 L13 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What makes the flows normalizing? Why can we treat the transformed function q_K as the probability density of z_K?</p> </li> <li> <p>Why do we require the transformations to be invertible?</p> </li> <li> <p>What do flows do better than variational inference we have seen so far?</p> </li> <li> <p>What can a probabilistic program do that VAE cannot?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w7-l14-p1">W7 L14 P1</h4> <ul> <li> <p>Explain what weighted model counting is and how is it related to probabilistic inference.</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> </ul> <h4 id="w8-l15-p1">W8 L15 P1</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What does the prior represent in this work?</p> </li> <li> <p>What is the posterior distribution representing?</p> </li> <li> <p>What does the likelihood term (probability of x given y) represent?</p> </li> <li> <p>what are the observations that we can observe?</p> </li> <li> <p>What did the authors need to change in inference procedures to make them work on this problem?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> <h4 id="w8-l15-p2">W8 L15 P2</h4> <ul> <li> <p>What is the problem that the work addresses?</p> </li> <li> <p>Explain the main contribution of the work in your own words.</p> </li> <li> <p>What does the prior represent in this work?</p> </li> <li> <p>What is the posterior distribution representing?</p> </li> <li> <p>What does the likelihood term (probability of x given y) represent?</p> </li> <li> <p>what are the observations that we can observe?</p> </li> <li> <p>What did the authors need to change in inference procedures to make them work on this problem?</p> </li> <li> <p>What question(s) would you ask yourself to test your understanding? Provide the question and the answer.</p> </li> <li> <p>Identify at least one limitation and, if you can, propose a solution.</p> </li> </ul> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Sebastijan Dumancic. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>